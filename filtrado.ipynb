{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encodes a dictionary into a jason\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Special json encoder for numpy types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "\n",
    "def jasonGenerator(path,results,photo_name):\n",
    "    \"\"\" Generates a .jason file in the path given folder from the a dictionary \"\"\"\n",
    "    with open(path + '/analysis_result_' + photo_name + '.json',\"w\") as miarch:\n",
    "        miarch.write(json.dumps(results, cls=NumpyEncoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jasonReader(path):\n",
    "    \"\"\" Reads the .jason generated with jasonGenartor the given path\"\"\"\n",
    "    with open(path,'r') as miarch:\n",
    "        loaded_dict = json.loads(miarch.read())\n",
    "    return loaded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicesParser_max_peaksI(dictres):\n",
    "#extract all the slices data\n",
    "    allSlices = dictres['slices']\n",
    "    #seting columns names\n",
    "    column_names = ['transient' + str(x) for x in range(0,len(allSlices[0]['max_peaks_intensities']))]    \n",
    "    #define a dataframe\n",
    "    df_sum = pd.DataFrame(columns=column_names)\n",
    "    for i in range(0,len(allSlices)):\n",
    "        df_sum.loc[i] = allSlices[i]['max_peaks_intensities'][0:] \n",
    "    return df_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicesParser_halfpeakstime(dictres):\n",
    "    #extract all the slices data\n",
    "    allSlices = dictres['slices']\n",
    "    \n",
    "    #seting columns names\n",
    "    column_names = ['transient' + str(x+1) for x in range(0,len(allSlices[0]['times_to_half_peaks']))]\n",
    "    #define a dataframe\n",
    "    df_sum = pd.DataFrame(columns=column_names)\n",
    "    #populating the dataframe\n",
    "    for i in range(0,len(allSlices)):\n",
    "        df_sum.loc[i+1] = allSlices[i]['times_to_half_peaks']\n",
    "    return df_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicesParser_amplitudes(dictres):\n",
    "    #extract all the slices data\n",
    "    allSlices = dictres['slices']\n",
    "    #seting columns names\n",
    "    column_names = ['transient' + str(x+1) for x in range(0,len(allSlices[0]['amplitudes']))]\n",
    "    #define a dataframe\n",
    "    df_sum = pd.DataFrame(columns=column_names)\n",
    "    #populating the dataframe\n",
    "    for i in range(0,len(allSlices)):\n",
    "        df_sum.loc[i+1] = allSlices[i]['amplitudes']\n",
    "\n",
    "    return df_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicesParser_min_peaksI(dictres):\n",
    "    #extract all the slices data\n",
    "    allSlices = dictres['slices']\n",
    "    #seting columns names\n",
    "    column_names = ['transient' + str(x+1) for x in range(0,len(allSlices[0]['min_peaks_intensities']))]\n",
    "    #define a dataframe\n",
    "    df_sum = pd.DataFrame(columns=column_names)\n",
    "    #populating the dataframe\n",
    "    for i in range(0,len(allSlices)):\n",
    "        df_sum.loc[i+1] = allSlices[i]['min_peaks_intensities']\n",
    "\n",
    "    return df_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicesParser_peaktime(dictres):\n",
    "    #extract all the slices data\n",
    "    allSlices = dictres['slices']\n",
    "\n",
    "    #seting columns names\n",
    "    column_names = ['transient' + str(x+1) for x in range(0,len(allSlices[0]['times_to_peaks']))]\n",
    "    #define a dataframe\n",
    "    df_sum = pd.DataFrame(columns=column_names)\n",
    "    #populating the dataframe\n",
    "    for i in range(0,len(allSlices)):\n",
    "        df_sum.loc[i+1] = allSlices[i]['times_to_peaks']\n",
    "\n",
    "    return df_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcula el alternance ratio\n",
    "\n",
    "AR_list = []\n",
    "path = '/media/leandro/Volumen1TB/Lean/Analizador_imagenes_calcio/Luis/jsons/'\n",
    "jsons_list = os.listdir(path)\n",
    "for file in jsons_list:\n",
    "    path_file = path + file\n",
    "    dic = jasonReader(path_file)\n",
    "    slices = dic['slices']\n",
    "    i = 0\n",
    "    df = pd.DataFrame(columns=['transient1','transient2','transient_max'])\n",
    "    print (file)\n",
    "    for i in range(0,33):\n",
    "        try:\n",
    "            amps = slices[i]['amplitudes'][0:2]\n",
    "            amps.append(max(amps))\n",
    "            i += 1\n",
    "            df.loc[len(df)] = amps\n",
    "        except IndexError:\n",
    "            pass\n",
    "    df.to_csv(path + file + 'max_amp.csv', sep='\\t')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Guarda en tablas separadas los datos de cada json por parámetro\n",
    "\n",
    "param_list = ['max_peaks_positions','min_peaks_positions','max_peaks_intensities','min_peaks_intensities','amplitudes','times_to_peaks','times_to_half_peaks']\n",
    "\n",
    "for param in param_list:\n",
    "    os.makedirs(path2 + param + '/')\n",
    "    path = '/media/leandro/Volumen1TB/Lean/Analizador_imagenes_calcio/Luis/jsons/'\n",
    "    path2 = '/media/leandro/Volumen1TB/Lean/Analizador_imagenes_calcio/Luis/'\n",
    "    jsons_list = os.listdir(path)\n",
    "    for file in jsons_list:\n",
    "        path_file = path + file\n",
    "        dic = jasonReader(path_file)\n",
    "        slices = dic['slices']\n",
    "        i = 0\n",
    "        df = pd.DataFrame(columns=['transient1','transient2'])\n",
    "        print (file)\n",
    "        for i in range(0,33):\n",
    "            try:\n",
    "                amps = slices[i][param][0:2]\n",
    "                i += 1\n",
    "                df.loc[len(df)] = amps\n",
    "            except IndexError:\n",
    "                pass\n",
    "        df.to_csv(path2 + param + '/' + file + param + '.csv', sep='\\t')\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guarda en una tabla los datos de la célula entera\n",
    "\n",
    "path = '/media/leandro/Volumen1TB/Lean/Analizador_imagenes_calcio/Luis/jsons/'\n",
    "path2 = '/media/leandro/Volumen1TB/Lean/Analizador_imagenes_calcio/Luis/'\n",
    "jsons_list = os.listdir(path)\n",
    "os.makedirs(path2 + 'whole_cell/')\n",
    "for file in jsons_list:\n",
    "    path_file = path + file\n",
    "    dic = jasonReader(path_file)\n",
    "    dic = dic['image']\n",
    "    pairs = {k: dic[k] for k in list(dic)[1:]}\n",
    "    keys = list(pairs.keys())\n",
    "    df = pd.DataFrame()\n",
    "    for label in range(0,len(pairs)):\n",
    "        df[keys[label]] = pairs[keys[label]][0:2]\n",
    "    df = df.T\n",
    "    df.columns = ['Transient1', 'Transient2']\n",
    "    df.to_csv(path2 + 'whole_cell/' + file + '.csv', sep='\\t')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "050320c8_analysis_result_8b000.json\n",
      "61 63\n",
      "295 51\n",
      "60 64\n",
      "296 58\n",
      "61 63\n",
      "295 56\n",
      "61 64\n",
      "295 56\n",
      "60 62\n",
      "295 56\n",
      "62 61\n",
      "296 55\n",
      "61 62\n",
      "294 57\n",
      "62 65\n",
      "296 55\n",
      "61 66\n",
      "295 56\n",
      "62 65\n",
      "296 58\n",
      "62 64\n",
      "296 53\n",
      "62 65\n",
      "296 55\n",
      "63 60\n",
      "297 55\n",
      "65 59\n",
      "298 55\n",
      "66 55\n",
      "299 53\n",
      "63 54\n",
      "297 56\n",
      "62 54\n",
      "296 52\n",
      "62 55\n",
      "297 51\n",
      "61 60\n",
      "296 55\n",
      "62 60\n",
      "300 52\n",
      "62 60\n",
      "298 54\n",
      "62 59\n",
      "297 56\n",
      "61 61\n",
      "297 59\n",
      "63 61\n",
      "298 58\n",
      "62 64\n",
      "296 57\n",
      "62 65\n",
      "296 55\n",
      "62 69\n",
      "296 59\n",
      "62 66\n",
      "296 52\n",
      "62 67\n",
      "296 55\n",
      "61 61\n",
      "296 57\n",
      "61 61\n",
      "296 57\n",
      "60 63\n",
      "296 59\n",
      "61 64\n",
      "295 59\n",
      "60 68\n",
      "296 55\n",
      "61 62\n",
      "295 57\n",
      "61 64\n",
      "295 58\n",
      "61 69\n",
      "297 58\n",
      "61 73\n",
      "297 60\n",
      "62 66\n",
      "297 58\n",
      "62 65\n",
      "296 53\n",
      "61 59\n",
      "296 49\n",
      "61 54\n",
      "296 50\n"
     ]
    }
   ],
   "source": [
    "# Cálculo del TAU67%\n",
    "\n",
    "TAU = pd.DataFrame()\n",
    "path = '/media/leandro/Volumen1TB/Lean/Analizador_imagenes_calcio/Luis/jsons/'\n",
    "path2 = '/media/leandro/Volumen1TB/Lean/Analizador_imagenes_calcio/Luis/'\n",
    "jsons_list = os.listdir(path)\n",
    "# os.makedirs(path2 + 'whole_cell/')\n",
    "for file in jsons_list[0:1]:\n",
    "    path_file = path + file\n",
    "    dic = jasonReader(path_file)\n",
    "    dic = dic['slices']\n",
    "    for peak in range  (0, len(dic)):\n",
    "        amp = ((np.array(dic[peak]['max_peaks_intensities'][0:2]) - np.array(dic[peak] ['min_peaks_intensities'][1:3]))/np.exp(1))+np.array(dic[peak] ['min_peaks_intensities'][1:3])\n",
    "        for i in range(0,len(amp)):\n",
    "            trans_list = dic[peak]['intensities'][dic[peak]['max_peaks_positions'][i]:dic[peak]['min_peaks_positions'][i+1]]\n",
    "            print(dic[peak]['max_peaks_positions'][i],trans_list.index(int(amp[i])))\n",
    "#             ttp50 = (np.abs(lista_up - amp50[i])).argmin()\n",
    "#             lista_ttp50.append (ttp50)\n",
    "#         tiempo_50pico ['Pico_' + str(peak)] = lista_ttp50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalcula la amplitud normalizando por el baseline de 1Hz\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
