{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from peakutils import indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jasonReader(path):\n",
    "    \"\"\" Reads the .jason generated with jasonGenartor the given path\"\"\"\n",
    "    with open(path,'r') as miarch:\n",
    "        loaded_dict = json.loads(miarch.read())\n",
    "    return loaded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wholecellParser(dictres):\n",
    "    #extract all the slices data\n",
    "    allSlices = dictres['image']\n",
    "    #seting columns names\n",
    "    column_names = ['max_peaks_positions','max_peaks_intensities','min_peaks_positions','min_peaks_intensities','amplitudes','times_to_peaks','tau_s']\n",
    "    #define a dataframe\n",
    "    df_sum = pd.DataFrame(columns=column_names)\n",
    "    #populating the dataframe\n",
    "    for i in column_names:\n",
    "        df_sum.loc[:,i] = allSlices[i][:2]\n",
    "\n",
    "    return df_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivate_date(array):\n",
    "    return np.gradient(array, axis=None, edge_order=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " def savitzky_golay(y, window_size, order, deriv=0, rate=1):\n",
    "    r\"\"\"Smooth (and optionally differentiate) data with a Savitzky-Golay filter.\n",
    "    The Savitzky-Golay filter removes high frequency noise from data.\n",
    "    It has the advantage of preserving the original shape and\n",
    "    features of the signal better than other types of filtering\n",
    "    approaches, such as moving averages techniques.\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like, shape (N,)\n",
    "        the values of the time history of the signal.\n",
    "    window_size : int\n",
    "  12         the length of the window. Must be an odd integer number.\n",
    "  13     order : int\n",
    "  14         the order of the polynomial used in the filtering.\n",
    "  15         Must be less then `window_size` - 1.\n",
    "  16     deriv: int\n",
    "  17         the order of the derivative to compute (default = 0 means only smoothing)\n",
    "  18     Returns\n",
    "  19     -------\n",
    "  20     ys : ndarray, shape (N)\n",
    "  21         the smoothed signal (or it's n-th derivative).\n",
    "  22     Notes\n",
    "  23     -----\n",
    "  24     The Savitzky-Golay is a type of low-pass filter, particularly\n",
    "  25     suited for smoothing noisy data. The main idea behind this\n",
    "  26     approach is to make for each point a least-square fit with a\n",
    "  27     polynomial of high order over a odd-sized window centered at\n",
    "  28     the point.\n",
    "  29     Examples\n",
    "  30     --------\n",
    "  31     t = np.linspace(-4, 4, 500)\n",
    "  32     y = np.exp( -t**2 ) + np.random.normal(0, 0.05, t.shape)\n",
    "  33     ysg = savitzky_golay(y, window_size=31, order=4)\n",
    "  34     import matplotlib.pyplot as plt\n",
    "  35     plt.plot(t, y, label='Noisy signal')\n",
    "  36     plt.plot(t, np.exp(-t**2), 'k', lw=1.5, label='Original signal')\n",
    "  37     plt.plot(t, ysg, 'r', label='Filtered signal')\n",
    "  38     plt.legend()\n",
    "  39     plt.show()\n",
    "  40     References\n",
    "  41     ----------\n",
    "  42     .. [1] A. Savitzky, M. J. E. Golay, Smoothing and Differentiation of\n",
    "  43        Data by Simplified Least Squares Procedures. Analytical\n",
    "  44        Chemistry, 1964, 36 (8), pp 1627-1639.\n",
    "  45     .. [2] Numerical Recipes 3rd Edition: The Art of Scientific Computing\n",
    "  46        W.H. Press, S.A. Teukolsky, W.T. Vetterling, B.P. Flannery\n",
    "  47        Cambridge University Press ISBN-13: 9780521880688\n",
    "  48     \"\"\"\n",
    "    import numpy as np\n",
    "    from math import factorial\n",
    "    \n",
    "    try:\n",
    "        window_size = np.abs(int(window_size))\n",
    "        order = np.abs(int(order))\n",
    "    except ValueError as msg:\n",
    "        raise ValueError(\"window_size and order have to be of type int\")\n",
    "    if window_size % 2 != 1 or window_size < 1:\n",
    "        raise TypeError(\"window_size size must be a positive odd number\")\n",
    "    if window_size < order + 2:\n",
    "        raise TypeError(\"window_size is too small for the polynomials order\")\n",
    "    order_range = range(order+1)\n",
    "    half_window = (window_size -1) // 2\n",
    "    # precompute coefficients\n",
    "    b = np.mat([[k**i for i in order_range] for k in range(-half_window, half_window+1)])\n",
    "    m = np.linalg.pinv(b).A[deriv] * rate**deriv * factorial(deriv)\n",
    "    # pad the signal at the extremes with\n",
    "    # values taken from the signal itself\n",
    "    firstvals = y[0] - abs( y[1:half_window+1][::-1] - y[0] )\n",
    "    lastvals = y[-1] + abs(y[-half_window-1:-1][::-1] - y[-1])\n",
    "    y = np.concatenate((firstvals, y, lastvals))\n",
    "    return np.convolve( m[::-1], y, mode='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicesParser_gradient(dictres,min_dist_between_max_peak):\n",
    "    #extract all the slices data\n",
    "    allSlices = dictres['slices']\n",
    "    #seting columns names\n",
    "    column_names = ['transient' + str(x+1) for x in range(0,len(dictres['image']['max_peaks_positions']))]\n",
    "    #define a dataframe\n",
    "    df_sum = pd.DataFrame(columns=column_names)\n",
    "    for _slice in range(len(allSlices)):\n",
    "        slice_intensities = allSlices[_slice]['intensities']\n",
    "        #calculate gradient\n",
    "        yhat = savitzky_golay(np.array(slice_intensities), 25, int(len(allSlices[_slice]['intensities'])/min_dist_between_max_peak),1)\n",
    "        #Maximun gradient\n",
    "        amp_list = _max_peaks_positions(yhat, min_dist_between_max_peak)\n",
    "        gradient_list = _intensities_in_positions(yhat, amp_list)[:len(dictres['image']['max_peaks_positions'])]\n",
    "        if len(gradient_list)==len(dictres['image']['max_peaks_positions']):\n",
    "            #populating the dataframe\n",
    "            df_sum.loc[_slice] = gradient_list\n",
    "        else:\n",
    "            pass\n",
    "    return df_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _max_peaks_positions(vector, min_dist_between_max_peaks):\n",
    "    threshold = 0.2\n",
    "    possible_max_peaks = indexes(np.array(vector), thres=threshold, min_dist=min_dist_between_max_peaks)\n",
    "    intensity_avg = sum(vector) / len(vector)\n",
    "    max_peaks = [max_peak for max_peak in possible_max_peaks if vector[max_peak] > intensity_avg/8]\n",
    "\n",
    "    if len(max_peaks) == 0: raise ValueError(\"Peaks not found\")\n",
    "\n",
    "    return max_peaks\n",
    "\n",
    "def _analyze_matrix_slice(matrix, max_peaks_positions):\n",
    "    intensities = np.asarray(_mean_columns(matrix), dtype=np.int16)\n",
    "    max_peaks_intensities = _intensities_in_positions(intensities, max_peaks_positions)\n",
    "\n",
    "    return ({\n",
    "        \"max_peaks_intensities\": max_peaks_intensities,\n",
    "\n",
    "    })\n",
    "def _intensities_in_positions(intensities, positions):\n",
    "    return [intensities[position] for position in positions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discordance_index(path,min_dist_between_max_peaks):\n",
    "    DIs = pd.DataFrame()\n",
    "    for j in range (1,len(slicesParser_gradient(path,min_dist_between_max_peaks).columns)):\n",
    "        rel_diff = []\n",
    "        for i in range (1, len (slicesParser_gradient(path,min_dist_between_max_peaks))):\n",
    "            T1 = list(slicesParser_gradient(path,min_dist_between_max_peaks)['transient'+str(j)])[i]\n",
    "            T2 = list(slicesParser_gradient(path,min_dist_between_max_peaks)['transient'+str(j+1)])[i]\n",
    "            rel_diff.append((T1-T2)/max(T1,T2))\n",
    "        DIs[j] = rel_diff\n",
    "    return DIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_imagenes = '/media/leandro/Volumen1TB/Lean/Analizador_imagenes_calcio/Luis/seleccion/'\n",
    "df_tabulado = pd.read_csv(path_imagenes + 'tabulado_imagenes.csv', index_col= 'Unnamed: 0')\n",
    "\n",
    "dict_min_dist = {1:200,3:70,4:50,5:40} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5\n",
       "Name: frecuencia, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tabulado[(df_tabulado['experimento']==160712) &  (df_tabulado['foto'].isin(['5b000']))]['frecuencia'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37_040412_c3b000.json\n",
      "37_030412_c1e000.json\n",
      "37_200712_3c001.json\n",
      "37_160712_5b000.json\n",
      "37_030412_c4b000.json\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 0 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-2fd7d7bcd871>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mexperiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfoto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mphoto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfoto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mfrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_tabulado\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_tabulado\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'experimento'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m  \u001b[0;34m(\u001b[0m\u001b[0mdf_tabulado\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'foto'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphoto\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'frecuencia'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mmin_dist_between_max_peaks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_min_dist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfrec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjasonReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfoto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m         if (\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 991\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    992\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    346\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_range\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "path = '/media/leandro/Volumen1TB/Lean/Analizador_imagenes_calcio/Luis/imagenes/37°/'\n",
    "df = pd.DataFrame()\n",
    "for foto in os.listdir(path):\n",
    "    if 'json' in foto:\n",
    "        print(foto)\n",
    "        experiment = foto.split('_')[1][:6]\n",
    "        photo = foto.split('_')[-1].split('.')[0]\n",
    "        frec = df_tabulado[(df_tabulado['experimento'].isin([experiment])) &  (df_tabulado['foto'].isin([photo]))]['frecuencia'].reset_index(drop=True)[0]\n",
    "        min_dist_between_max_peaks = dict_min_dist[frec]\n",
    "        array = jasonReader(path + foto)\n",
    "        discordance_list = discordance_index(array,min_dist_between_max_peaks).std()\n",
    "        df[foto[:-4]] = discordance_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37_040412_c3b000.</th>\n",
       "      <td>0.149501</td>\n",
       "      <td>0.186963</td>\n",
       "      <td>0.204308</td>\n",
       "      <td>0.191244</td>\n",
       "      <td>0.178368</td>\n",
       "      <td>0.191017</td>\n",
       "      <td>0.220627</td>\n",
       "      <td>0.205073</td>\n",
       "      <td>0.155650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37_030412_c1e000.</th>\n",
       "      <td>0.205983</td>\n",
       "      <td>0.278625</td>\n",
       "      <td>0.272091</td>\n",
       "      <td>0.268566</td>\n",
       "      <td>0.211374</td>\n",
       "      <td>0.210967</td>\n",
       "      <td>0.266985</td>\n",
       "      <td>0.185951</td>\n",
       "      <td>0.236752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37_200712_3c001.</th>\n",
       "      <td>0.323512</td>\n",
       "      <td>0.351240</td>\n",
       "      <td>0.290627</td>\n",
       "      <td>0.187210</td>\n",
       "      <td>0.255577</td>\n",
       "      <td>0.392940</td>\n",
       "      <td>0.404243</td>\n",
       "      <td>0.458584</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37_030412_c2b000.</th>\n",
       "      <td>0.078730</td>\n",
       "      <td>0.082961</td>\n",
       "      <td>0.100195</td>\n",
       "      <td>0.104464</td>\n",
       "      <td>0.082907</td>\n",
       "      <td>0.085012</td>\n",
       "      <td>0.119724</td>\n",
       "      <td>0.127776</td>\n",
       "      <td>0.106136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37_160712_3b000.</th>\n",
       "      <td>0.496466</td>\n",
       "      <td>0.356092</td>\n",
       "      <td>0.361485</td>\n",
       "      <td>0.401220</td>\n",
       "      <td>0.377912</td>\n",
       "      <td>0.342374</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37_270312_270312g0.</th>\n",
       "      <td>0.119073</td>\n",
       "      <td>0.157942</td>\n",
       "      <td>0.149512</td>\n",
       "      <td>0.165023</td>\n",
       "      <td>0.144157</td>\n",
       "      <td>0.130750</td>\n",
       "      <td>0.147304</td>\n",
       "      <td>0.116558</td>\n",
       "      <td>0.150579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37_180712_1e001.</th>\n",
       "      <td>0.254827</td>\n",
       "      <td>0.237044</td>\n",
       "      <td>0.304111</td>\n",
       "      <td>0.302375</td>\n",
       "      <td>0.266040</td>\n",
       "      <td>0.218845</td>\n",
       "      <td>0.235832</td>\n",
       "      <td>0.268762</td>\n",
       "      <td>0.262435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37_180712_2b001.</th>\n",
       "      <td>0.186039</td>\n",
       "      <td>0.137699</td>\n",
       "      <td>0.146021</td>\n",
       "      <td>0.158181</td>\n",
       "      <td>0.145770</td>\n",
       "      <td>0.166565</td>\n",
       "      <td>0.155324</td>\n",
       "      <td>0.147470</td>\n",
       "      <td>0.149863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37_030412_c3b000.</th>\n",
       "      <td>0.087517</td>\n",
       "      <td>0.103791</td>\n",
       "      <td>0.127754</td>\n",
       "      <td>0.115376</td>\n",
       "      <td>0.098874</td>\n",
       "      <td>0.105438</td>\n",
       "      <td>0.128471</td>\n",
       "      <td>0.124682</td>\n",
       "      <td>0.103213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            1         2         3         4         5  \\\n",
       "37_040412_c3b000.    0.149501  0.186963  0.204308  0.191244  0.178368   \n",
       "37_030412_c1e000.    0.205983  0.278625  0.272091  0.268566  0.211374   \n",
       "37_200712_3c001.     0.323512  0.351240  0.290627  0.187210  0.255577   \n",
       "37_030412_c2b000.    0.078730  0.082961  0.100195  0.104464  0.082907   \n",
       "37_160712_3b000.     0.496466  0.356092  0.361485  0.401220  0.377912   \n",
       "37_270312_270312g0.  0.119073  0.157942  0.149512  0.165023  0.144157   \n",
       "37_180712_1e001.     0.254827  0.237044  0.304111  0.302375  0.266040   \n",
       "37_180712_2b001.     0.186039  0.137699  0.146021  0.158181  0.145770   \n",
       "37_030412_c3b000.    0.087517  0.103791  0.127754  0.115376  0.098874   \n",
       "\n",
       "                            6         7         8         9  \n",
       "37_040412_c3b000.    0.191017  0.220627  0.205073  0.155650  \n",
       "37_030412_c1e000.    0.210967  0.266985  0.185951  0.236752  \n",
       "37_200712_3c001.     0.392940  0.404243  0.458584       NaN  \n",
       "37_030412_c2b000.    0.085012  0.119724  0.127776  0.106136  \n",
       "37_160712_3b000.     0.342374       NaN       NaN       NaN  \n",
       "37_270312_270312g0.  0.130750  0.147304  0.116558  0.150579  \n",
       "37_180712_1e001.     0.218845  0.235832  0.268762  0.262435  \n",
       "37_180712_2b001.     0.166565  0.155324  0.147470  0.149863  \n",
       "37_030412_c3b000.    0.105438  0.128471  0.124682  0.103213  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T.to_csv('/media/leandro/Volumen1TB/Lean/Analizador_imagenes_calcio/Luis/analisis/DI_gradient_37.csv',decimal = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precisión y exactitud amplitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/media/leandro/Volumen1TB/Lean/Analizador_imagenes_calcio/Luis/jsons/'\n",
    "file_tabulado = pd.read_excel(path + 'tab_prueba2.xlsx')\n",
    "del file_tabulado['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = pd.read_csv(\"/media/leandro/Volumen1TB/Lean/Analizador_imagenes_calcio/Luis/DI_manual_1hz.csv\", sep=\",\")\n",
    "df_manual.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/media/leandro/Volumen1TB/Lean/Analizador_imagenes_calcio/Luis/analisis/'\n",
    "experiments = os.listdir(path + 'jsons/')\n",
    "dif_amp = pd.DataFrame()\n",
    "dias = df_manual['experimento'].drop_duplicates()\n",
    "foto = df_manual['foto'].drop_duplicates()\n",
    "for j in dias:\n",
    "    for k in foto:\n",
    "        for i in range(0,len (experiments)):\n",
    "            if (str(j) in experiments[i]) & (k in experiments[i]):\n",
    "                try:\n",
    "                    experiment = jasonReader(path + 'jsons/' + experiments[i])\n",
    "                    df_file = wholecellParser(experiment)\n",
    "                    df_file['file'] = experiments[i]\n",
    "                    dif_amp = dif_amp.append(df_file, ignore_index=True)\n",
    "                except:\n",
    "                    print(experiments[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dif_amp.to_csv('/media/leandro/Volumen1TB/Lean/Analizador_imagenes_calcio/Luis/analisis/amp_cadiap_5hz.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_cardiap = pd.read_csv('/media/leandro/Volumen1TB/Lean/Analizador_imagenes_calcio/Luis/analisis/amp_cadiap.csv')\n",
    "amp_cardiap.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "\n",
    "x = amp_cardiap['TTP_norm'].dropna()\n",
    "y = df_manual ['TTP_manual'].dropna()\n",
    "scipy.stats.pearsonr(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame((x,y)).T\n",
    "def outliers(df,column,percentiles):\n",
    "    Q1 = df[column].quantile(percentiles[0])\n",
    "    Q3 = df[column].quantile(percentiles[1])\n",
    "    IQR = Q3 - Q1\n",
    "    idx = ~((df[column] < (Q1 - 1.5 * IQR)) | (df[column] > (Q3 + 1.5 * IQR)))\n",
    "    df = df[idx.values]\n",
    "    return df\n",
    "\n",
    "df = outliers(df,'TTP_norm',[0.02,0.98])\n",
    "df = outliers(df,'TTP_manual',[0.02,0.98])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pp\n",
    "\n",
    "# plot\n",
    "pp.scatter(df['TTP_norm'],df['TTP_manual'])\n",
    "pp.xlabel('times_to_peaks-CardIAP')\n",
    "pp.ylabel('TTP-manual')\n",
    "\n",
    "# pp.savefig('/media/leandro/Volumen1TB/Lean/Analizador_imagenes_calcio/Luis/analisis/TTP_cadiap_1hz.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.metrics import r2_score\n",
    "#regression part\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "\n",
    "line = slope*x+intercept\n",
    "lines = plt.plot(x, line, 'cornflowerblue', label='y={:.2f}x+{:.2f}'.format(slope,intercept))\n",
    "#end\n",
    "plt.setp(lines, color='cornflowerblue', linewidth=2.0,linestyle = ':')\n",
    "plt.scatter(x,y, color=\"k\", s=3.5)\n",
    "plt.legend(fontsize=9)\n",
    "plt.annotate(\"r-squared = {:.3f}\".format(r2_score(x, y)), (0, 1))\n",
    "# pp.savefig('/media/leandro/Volumen1TB/Lean/Analizador_imagenes_calcio/Luis/analisis/TTP_cadiap_1hz.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import variance\n",
    "\n",
    "diff = x[:48]-y[:48]\n",
    "print(diff.mean())\n",
    "print(diff.std())\n",
    "print(x[:48].mean(),x[:48].std(),variance(x[:48]),y[:48].mean(),y[:48].std(),variance(y[:48]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_1samp(diff, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.levene(x[:48],y[:48],center='trimmed', proportiontocut=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_ar = '/media/leandro/Volumen1TB/Lean/Analizador_imagenes_calcio/Luis/analisis/ARs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tabulado = '/media/leandro/Volumen1TB/Lean/Analizador_imagenes_calcio/Luis/seleccion/'\n",
    "file_tabulado = pd.read_csv(path_tabulado + 'tabulado_imagenes.csv')\n",
    "del file_tabulado['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = os.listdir(path_ar)\n",
    "dif_amp = []\n",
    "dias = file_tabulado['experimento'].drop_duplicates()\n",
    "foto = file_tabulado['foto'].drop_duplicates()\n",
    "for j in dias:\n",
    "    for k in foto:\n",
    "        for i in range(0,len (experiments)):\n",
    "            if (str(j) in experiments[i]) & (k in experiments[i]):\n",
    "                file_tabulado['experimento']\n",
    "                ar = pd.read_csv(path_ar + experiments[i],sep='\\t',decimal=',')\n",
    "                dif_amp.append([experiments[i],ar['local AR'].mean()])\n",
    "                index = file_tabulado.index[(file_tabulado['experimento'] == j) & (file_tabulado['foto'] == k)].tolist()\n",
    "                file_tabulado.loc[index,'AR'] = ar['local AR'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_tabulado[(file_tabulado['tratamiento']=='vk') & (file_tabulado['AR']<0.3) & (file_tabulado['frecuencia']==5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_tabulado[(file_tabulado['tratamiento']=='c') & (file_tabulado['frecuencia']==1)]['AR'].plot.hist(bins=20, alpha=0.5)\n",
    "file_tabulado[(file_tabulado['tratamiento']=='vk') & (file_tabulado['frecuencia']==5)]['AR'].plot.hist(bins=20, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_tabulado[(file_tabulado['tratamiento']=='c') & (file_tabulado['frecuencia']==1)]['AR'].median())\n",
    "print(file_tabulado[(file_tabulado['tratamiento']=='c') & (file_tabulado['frecuencia']==1)]['AR'].std())\n",
    "print(file_tabulado[(file_tabulado['tratamiento']=='vk') & (file_tabulado['frecuencia']==5)]['AR'].median())\n",
    "print(file_tabulado[(file_tabulado['tratamiento']=='vk') & (file_tabulado['frecuencia']==5)]['AR'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "stats.ks_2samp(file_tabulado[(file_tabulado['tratamiento']=='c') & (file_tabulado['frecuencia']==5)]['AR'], file_tabulado[(file_tabulado['tratamiento']=='vk') & (file_tabulado['frecuencia']==5)]['AR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DICRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = pd.read_csv('/media/leandro/Volumen1TB/Lean/Analizador_imagenes_calcio/Luis/seleccion/tabulado_imagenes.csv', encoding='utf-8')\n",
    "del tab['Unnamed: 0']\n",
    "pd.set_option('display.max_rows', None)\n",
    "controles = tab[tab['tratamiento'] == 'c']\n",
    "controles = controles.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exps = os.listdir(path + 'jsons/')\n",
    "experiments = []\n",
    "for j in range(0,len(exps)):\n",
    "    for i in range(0,len(controles)):\n",
    "        if str(controles['experimento'][i]) in exps[j]:\n",
    "            if str(controles['foto'][i]) in exps[j]:\n",
    "                experiments.append(exps[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/media/leandro/Volumen1TB/Lean/Analizador_imagenes_calcio/Luis/analisis/'\n",
    "# experiments = os.listdir(path + 'jsons/')\n",
    "complete_file = pd.DataFrame()\n",
    "for i in range(0,len (experiments)):\n",
    "    try:\n",
    "        experiment = jasonReader(path + 'jsons/' + experiments[i])\n",
    "        amp_files = slicesParser_amplitudes(experiment).filter(['transient1','transient2'])\n",
    "        amp_files = amp_files[:][0:33]\n",
    "        transient_max = []\n",
    "        for j in range(1,len(amp_files)+1):\n",
    "            transient_max.append(max(amp_files['transient1'][j],amp_files['transient2'][j]))\n",
    "        amp_files['transient_max'] = transient_max\n",
    "        amp_files.loc[len(amp_files)+1] = (amp_files.mean())\n",
    "        amp_files.loc[len(amp_files)+1] = [amp_files['transient1'][len(amp_files)]*100/amp_files['transient_max'][len(amp_files)],amp_files['transient2'][len(amp_files)]*100/amp_files['transient_max'][len(amp_files)],'']\n",
    "        amp_files[str(experiments[i])] = list(range(1,len(amp_files)-1))+['promedios']+['porcentaje_max']\n",
    "        complete_file = pd.concat([complete_file,amp_files], axis=1)\n",
    "    except ValueError:\n",
    "        print(experiments[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_file.to_excel(path + 'DICRI_controles.xlsx', engine='xlsxwriter')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amplitud de transitorio global vs DI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/media/leandro/Volumen1TB/Lean/Analizador_imagenes_calcio/Luis/analisis/json_nuevos/'\n",
    "file_tabulado = pd.read_csv(path + 'tabulado_parceado.csv')\n",
    "file_tabulado.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = jasonReader(path + 'jsons/050320c8_analysis_result_8b000.json')\n",
    "allSlices = experiment['slices']\n",
    "for i in range(0,len(allSlices)):\n",
    "    print(allSlices[i]['amplitudes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/media/leandro/Volumen1TB/Lean/Analizador_imagenes_calcio/Luis/analisis/json_nuevos/whole_cell/'\n",
    "experiments = os.listdir(path)\n",
    "df = pd.DataFrame()\n",
    "for experiment in experiments:\n",
    "    df = pd.read_csv(path + experiment, sep = '\\t', index_col=0).loc['amplitudes']\n",
    "    for imagen in range(0,len(file_tabulado)):\n",
    "        cel = file_tabulado['célula'].loc[imagen]\n",
    "        foto = file_tabulado['foto'].loc[imagen]\n",
    "        if (cel in experiment) & (foto in experiment):\n",
    "            print(experiment,df)\n",
    "\n",
    "#     print (experiment, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
