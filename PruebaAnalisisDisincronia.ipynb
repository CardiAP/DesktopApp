{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En esta celda se puede probar la funcion de analisis de una imagen\n",
    "\n",
    "from lib.image import image_data\n",
    "from lib.analysis import dyssynchonia_analysis\n",
    "import cv2\n",
    "\n",
    "# Path completo donde esta la imagen (incluyendo nombre y extencion)\n",
    "path = 'photos_examples/c1d000.tif'\n",
    "\n",
    "# Ancho de la feta a analisar (es un parametro opcional)\n",
    "ancho_corte = 5\n",
    "\n",
    "# Distancia minima en pixeles entre picos\n",
    "min_dist_between_maxs = 20\n",
    "\n",
    "image = cv2.imread(path)\n",
    "# Select ROI\n",
    "fromCenter = False\n",
    "showCrosshair = False\n",
    "seleted_parameters = cv2.selectROI(image, fromCenter, showCrosshair)\n",
    "\n",
    "# Crop image\n",
    "x_start = int(seleted_parameters[1])\n",
    "x_end = x_start + int(seleted_parameters[3])\n",
    "y_start = int(seleted_parameters[0])\n",
    "y_end = y_start + int(seleted_parameters[2])\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "image = image_data.get_image_data(image)\n",
    "image = image_data.crop_vertical(image, x_start, x_end)\n",
    "image = image_data.crop_horizontal(image, y_start, y_end)\n",
    "\n",
    "results = dyssynchonia_analysis.analyze_image(image, min_dist_between_maxs, ancho_corte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRUEBA ANALISIS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En esta celda se puede probar la funcion de analisis todas las imagenes .tif en un directorio\n",
    "import os\n",
    "from lib.image import image_data\n",
    "from lib.analysis import dyssynchonia_analysis\n",
    "import cv2\n",
    "\n",
    "# Ancho de la feta a analisar (es un parametro opcional)\n",
    "ancho_corte = 5\n",
    "\n",
    "# Distancia minima en pixeles entre picos\n",
    "min_dist_between_maxs = 20\n",
    "\n",
    "\n",
    "# Path donde estan las imagenes\n",
    "path = \"C:/Users/Leand/OneDrive/Documentos/Lean/Analizador_imagenes_calcio/seleccion/050320sel/050320c1\"\n",
    "\n",
    "images_paths = [ f'{path}/{file}' for file in os.listdir(path) if file.endswith(\".tif\") ]\n",
    "images = [ cv2.imread(image_path) for image_path in images_paths ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fromCenter = False\n",
    "showCrosshair = False\n",
    "\n",
    "#Tomamos la primer imagen para seleccionar el recorte elegido el resto de las imagenes se van a recortar igual\n",
    "seleted_parameters = cv2.selectROI(images[0], fromCenter, showCrosshair)\n",
    "\n",
    "# Crop image\n",
    "x_start = int(seleted_parameters[1])\n",
    "x_end = x_start + int(seleted_parameters[3])\n",
    "y_start = int(seleted_parameters[0])\n",
    "y_end = y_start + int(seleted_parameters[2])\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[8, 8, 8, ..., 8, 8, 9],\n",
       "        [8, 8, 8, ..., 8, 8, 8],\n",
       "        [8, 8, 8, ..., 8, 8, 8],\n",
       "        ...,\n",
       "        [8, 8, 8, ..., 8, 8, 8],\n",
       "        [8, 8, 8, ..., 8, 8, 8],\n",
       "        [8, 8, 8, ..., 8, 8, 8]], dtype=uint8),\n",
       " array([[13, 13, 13, ..., 12, 12, 13],\n",
       "        [13, 13, 13, ..., 12, 12, 12],\n",
       "        [13, 13, 13, ..., 12, 12, 12],\n",
       "        ...,\n",
       "        [13, 13, 13, ..., 18, 18, 18],\n",
       "        [13, 13, 13, ..., 18, 18, 18],\n",
       "        [13, 13, 13, ..., 18, 18, 18]], dtype=uint8),\n",
       " array([[16, 17, 17, ..., 19, 20, 20],\n",
       "        [16, 17, 17, ..., 19, 19, 19],\n",
       "        [16, 16, 17, ..., 18, 19, 19],\n",
       "        ...,\n",
       "        [12, 13, 13, ..., 13, 13, 13],\n",
       "        [12, 13, 13, ..., 13, 13, 13],\n",
       "        [12, 13, 13, ..., 13, 13, 13]], dtype=uint8),\n",
       " array([[ 9,  9,  9, ..., 24, 24, 24],\n",
       "        [ 9,  9,  9, ..., 23, 24, 24],\n",
       "        [ 9,  9,  9, ..., 23, 23, 23],\n",
       "        ...,\n",
       "        [ 8,  8,  9, ..., 15, 15, 15],\n",
       "        [ 8,  8,  9, ..., 15, 15, 15],\n",
       "        [ 8,  8,  9, ..., 15, 15, 15]], dtype=uint8)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = [ image_data.get_image_data(image) for image in images ]\n",
    "images = [ image_data.crop_vertical(image, x_start, x_end) for image in images ]\n",
    "images = [ image_data.crop_horizontal(image, y_start, y_end) for image in images ]\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_analysis = [dyssynchonia_analysis.analyze_image(image, min_dist_between_maxs, ancho_corte) for image in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + r'test.txt', 'w') as f:\n",
    "    f.write(\" \".join(map(str, dy_analysis)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis de datos\n",
    "Notas: \n",
    "- Image corresponde al análisis de toda la célula y slices corresponde a fetas de esa imagen\n",
    "- intensidades es el valor que resulta de comprimir (sumando) la matriz con las coordenadas de x,y,z de los pixeles \n",
    "- max_peaks_pos es la index de en la lista de intensidades \n",
    "- max_peaks_intensities el valor de intensidad que se corresponde con el pico en la posicion analoga de max_peaks_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encodes a dictionary into a jason\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Special json encoder for numpy types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "\n",
    "def jasonGenerator(path,results):\n",
    "    \"\"\" Generates a .jason file in the path given folder from the a dictionary \"\"\"\n",
    "    with open(path + '/analysis_result.jason',\"w\") as miarch:\n",
    "        miarch.write(json.dumps(reslust, cls=NumpyEncoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jasonReader(path):\n",
    "    \"\"\" Reads the .jason generated with jasonGenartor the given path\"\"\"\n",
    "    with open( path + '/analysis_result.json','r') as miarch:\n",
    "        loaded_dict = json.loads(miarch.read())\n",
    "    return loaded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def slicesParser_max_peaksI(path):\n",
    "    #read the analysis result \n",
    "    dictres = jasonReader(path)\n",
    "\n",
    "    #extract all the slices data\n",
    "    allSlices = dictres['slices']\n",
    "    \n",
    "    try:\n",
    "        #seting columns names\n",
    "        column_names = ['transient' + str(x) for x in range(1,len(allSlices[0]['max_peaks_intensities']))]\n",
    "\n",
    "        #define a dataframe\n",
    "        df_sum = pd.DataFrame(columns=column_names)\n",
    "\n",
    "        for i in range(1,len(allSlices)):\n",
    "            df_sum.loc[i+1] = allSlices[i]['max_peaks_intensities'][1:]\n",
    "\n",
    "        return df_sum\n",
    "    \n",
    "    except:\n",
    "        print('no possible callculation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicesParser_halfpeakstime(path):\n",
    "    #read the analysis result \n",
    "    dictres = jasonReader(path)\n",
    "    #extract all the slices data\n",
    "    allSlices = dictres['slices']\n",
    "    try:\n",
    "        #seting columns names\n",
    "        column_names = ['transient' + str(x+1) for x in range(0,len(allSlices[0]['times_to_half_peaks']))]\n",
    "        #define a dataframe\n",
    "        df_sum = pd.DataFrame(columns=column_names)\n",
    "        #populating the dataframe\n",
    "        for i in range(0,len(allSlices)):\n",
    "            df_sum.loc[i+1] = allSlices[i]['times_to_half_peaks']\n",
    "\n",
    "        return df_sum\n",
    "\n",
    "    except:\n",
    "        print('no possible callculation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicesParser_amplitudes(path):\n",
    "    #read the analysis result \n",
    "    dictres = jasonReader(path)\n",
    "    #extract all the slices data\n",
    "    allSlices = dictres['slices']\n",
    "    try:\n",
    "        #seting columns names\n",
    "        column_names = ['transient' + str(x+1) for x in range(0,len(allSlices[0]['amplitudes']))]\n",
    "        #define a dataframe\n",
    "        df_sum = pd.DataFrame(columns=column_names)\n",
    "        #populating the dataframe\n",
    "        for i in range(0,len(allSlices)):\n",
    "            df_sum.loc[i+1] = allSlices[i]['amplitudes']\n",
    "\n",
    "        return df_sum\n",
    "    \n",
    "    except:\n",
    "        print('no possible callculation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicesParser_min_peaksI(path):\n",
    "    #read the analysis result \n",
    "    dictres = jasonReader(path)\n",
    "    #extract all the slices data\n",
    "    allSlices = dictres['slices']\n",
    "    try:\n",
    "        #seting columns names\n",
    "        column_names = ['transient' + str(x+1) for x in range(0,len(allSlices[0]['min_peaks_intensities']))]\n",
    "        #define a dataframe\n",
    "        df_sum = pd.DataFrame(columns=column_names)\n",
    "        #populating the dataframe\n",
    "        for i in range(0,len(allSlices)):\n",
    "            df_sum.loc[i+1] = allSlices[i]['min_peaks_intensities']\n",
    "\n",
    "        return df_sum\n",
    "    \n",
    "    except:\n",
    "        print('no possible callculation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicesParser_peaktime(path):\n",
    "    #read the analysis result \n",
    "    dictres = jasonReader(path)\n",
    "    #extract all the slices data\n",
    "    allSlices = dictres['slices']\n",
    "\n",
    "    try:\n",
    "        #seting columns names\n",
    "        column_names = ['transient' + str(x+1) for x in range(0,len(allSlices[0]['times_to_peaks']))]\n",
    "        #define a dataframe\n",
    "        df_sum = pd.DataFrame(columns=column_names)\n",
    "        #populating the dataframe\n",
    "        for i in range(0,len(allSlices)):\n",
    "            df_sum.loc[i+1] = allSlices[i]['times_to_peaks']\n",
    "\n",
    "        return df_sum\n",
    "    \n",
    "    except:\n",
    "        print('no possible callculation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicesParser_tau(path):\n",
    "    #read the analysis result \n",
    "    dictres = jasonReader(path)\n",
    "    #extract all the slices data\n",
    "    allSlices = dictres['slices']\n",
    "    #seting columns names\n",
    "    try:\n",
    "        column_names = ['transient' + str(x+1) for x in range(0,len(allSlices[0]['tau_s']))]\n",
    "        #define a dataframe\n",
    "        df_sum = pd.DataFrame(columns=column_names)\n",
    "        #populating the dataframe\n",
    "        for i in range(0,len(allSlices)):\n",
    "            df_sum.loc[i+1] = allSlices[i]['tau_s']\n",
    "        \n",
    "        return df_sum\n",
    "    \n",
    "    except:\n",
    "        print('no possible callculation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BasicStasts(df):\n",
    "    if df.empty == False:\n",
    "        return df.astype('int').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discordIndex(df):\n",
    "    return df.iloc[2].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amplitudes_ratio(df):\n",
    "    df_alt = pd.DataFrame()\n",
    "    for i in range(0,len(list(df))-1):\n",
    "        name = 'ratio' + str(i+1) +'-' + str(i)\n",
    "        df_alt[name] = df[list(df)[i+1]]/df[list(df)[i]]\n",
    "    return df_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_arch = '/home/ana/Proyectos_Lean/CardiAP/DesktopApp'\n",
    "\n",
    "BasicStasts(slicesParser_max_peaksI(path_arch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitudes_ratio(slicesParser_amplitudes(path_arch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slicesParser_min_peaksI(path_arch).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "slicesParser_tau(path_arch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
