{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En esta celda se puede probar la funcion de analisis de una imagen\n",
    "\n",
    "from lib.image import image_data\n",
    "from lib.analysis import dyssynchrony_analysis\n",
    "import cv2\n",
    "\n",
    "# Path completo donde esta la imagen (incluyendo nombre y extencion)\n",
    "path = '/media/leandro/Volumen1TB/Lean/Analizador_imagenes_calcio/Luis/seleccion/170221/dan/'\n",
    "photo_name = 'D2e000'\n",
    "photo = photo_name + '.tif'\n",
    "# Ancho de la feta a analisar (es un parametro opcional)\n",
    "slice_width = 5\n",
    "\n",
    "# Distancia minima en pixeles entre picos\n",
    "min_dist_between_maxs = 40\n",
    "# Calibracion del tiempo de cada pixel\n",
    "calibration = 1\n",
    "\n",
    "image = cv2.imread(path + photo_name + '.tif')\n",
    "# Select ROI\n",
    "fromCenter = False\n",
    "showCrosshair = False\n",
    "seleted_parameters = cv2.selectROI(image, fromCenter, showCrosshair)\n",
    "\n",
    "# Crop image\n",
    "x_start = int(seleted_parameters[1])\n",
    "x_end = x_start + int(seleted_parameters[3])\n",
    "y_start = int(seleted_parameters[0]) \n",
    "y_end = y_start + int(seleted_parameters[2])\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "image = image_data.get_image_data(image)\n",
    "image = image_data.crop_vertical(image, x_start, x_end)\n",
    "image = image_data.crop_horizontal(image, y_start, y_end)\n",
    "\n",
    "results = dyssynchrony_analysis.analyze_image(image, min_dist_between_maxs, calibration, slice_width=slice_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "jasonGenerator(path,results,photo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot set a row with mismatched columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-9eb2146b1204>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjasonReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'analysis_result_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mphoto_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mslicesParser_amplitudes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-d0b94db99a53>\u001b[0m in \u001b[0;36mslicesParser_amplitudes\u001b[0;34m(dictres)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#populating the dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallSlices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mdf_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mallSlices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'amplitudes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf_sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0miloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"iloc\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m         \u001b[0miloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m   1624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1625\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1626\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1627\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_setitem_with_indexer_missing\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m   1856\u001b[0m                     \u001b[0;31m# must have conforming columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1857\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1858\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot set a row with mismatched columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot set a row with mismatched columns"
     ]
    }
   ],
   "source": [
    "dic = jasonReader(path + 'analysis_result_' + photo_name + '.json')\n",
    "slicesParser_amplitudes(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 74, 124, 169, 219, 263, 313, 357, 408, 451]\n",
      "0 [36, 128, 172, 222, 274, 317, 410, 503]\n",
      "1 [33, 130, 222, 271, 315, 369, 413, 460]\n",
      "2 [36, 132, 221, 263, 315, 412, 507]\n",
      "3 [35, 134, 222, 318, 413, 500]\n",
      "4 [35, 132, 221, 317, 413, 460, 507]\n",
      "5 [38, 130, 221, 264, 316, 367, 412, 503]\n",
      "6 [36, 135, 223, 315, 370, 411, 508]\n",
      "7 [38, 133, 220, 315, 369, 410, 508]\n",
      "8 [39, 130, 224, 315, 410, 458, 508]\n",
      "9 [38, 124, 175, 223, 314, 357, 409, 452]\n",
      "10 [33, 128, 222, 276, 319, 410, 509]\n",
      "11 [37, 84, 126, 174, 222, 316, 360, 414]\n",
      "12 [37, 125, 222, 315, 356, 410, 455]\n",
      "13 [36, 126, 169, 224, 314, 357, 410, 451, 506]\n",
      "14 [35, 128, 220, 322, 411, 505]\n",
      "15 [36, 129, 226, 314, 411, 507]\n",
      "16 [35, 125, 167, 226, 314, 410, 458, 507]\n",
      "17 [34, 122, 164, 219, 313, 411]\n",
      "18 [33, 124, 219, 261, 313, 370, 415, 475]\n",
      "19 [33, 75, 125, 220, 263, 315, 411]\n",
      "20 [36, 78, 129, 226, 314, 412]\n",
      "21 [37, 79, 125, 169, 219, 269, 312, 362, 413, 454, 502]\n",
      "22 [32, 78, 127, 170, 221, 266, 315, 356, 416]\n",
      "23 [36, 79, 128, 170, 228, 269, 317, 358, 408, 505]\n",
      "24 [32, 77, 126, 171, 218, 262, 313, 357, 411]\n",
      "25 [34, 128, 178, 220, 267, 322, 409, 502]\n",
      "26 [34, 76, 124, 172, 223, 267, 314, 361, 407, 448, 500]\n",
      "27 [38, 125, 173, 218, 267, 313, 361, 406, 451, 504]\n",
      "28 [37, 124, 172, 229, 315, 357, 406, 450, 504]\n",
      "29 [37, 124, 180, 221, 264, 313, 356, 408, 450, 500]\n",
      "30 [33, 83, 125, 172, 221, 264, 317, 358, 407, 449, 500]\n",
      "31 [32, 76, 122, 176, 219, 263, 315, 358, 406, 449, 501]\n",
      "32 [32, 76, 123, 171, 220, 265, 320, 362, 404, 451, 502]\n",
      "33 [37, 125, 171, 216, 265, 312, 360, 403, 452, 509]\n",
      "34 [31, 74, 125, 171, 217, 264, 312, 362, 404, 453, 506]\n",
      "35 [34, 125, 172, 217, 263, 313, 360, 404, 463, 506]\n",
      "36 [32, 75, 124, 174, 218, 264, 311, 358, 408, 457, 500]\n",
      "37 [31, 77, 125, 174, 218, 263, 311, 359, 407, 456, 501]\n",
      "38 [33, 76, 126, 174, 219, 268, 311, 359, 404, 455, 502]\n",
      "39 [33, 75, 125, 171, 217, 262, 310, 357, 408, 454, 502]\n",
      "40 [32, 78, 129, 170, 217, 265, 310, 364, 409, 451]\n",
      "41 [32, 77, 126, 170, 218, 265, 316, 362, 408, 452, 505]\n",
      "42 [31, 74, 124, 170, 216, 269, 313, 361, 407, 451, 506]\n",
      "43 [32, 77, 124, 171, 216, 267, 310, 363, 408, 452]\n",
      "44 [33, 76, 121, 172, 217, 270, 311, 354, 408, 452, 499]\n",
      "45 [31, 75, 121, 173, 218, 263, 316, 361, 405, 454, 504]\n",
      "46 [30, 75, 120, 171, 218, 265, 310, 363, 404, 453, 499]\n",
      "47 [31, 75, 122, 169, 216, 268, 311, 361, 402, 451, 497]\n",
      "48 [31, 74, 124, 172, 216, 264, 309, 362, 452, 499]\n",
      "49 [33, 75, 123, 172, 217, 266, 308, 360, 406, 453, 498]\n",
      "50 [28, 79, 122, 174, 216, 266, 308, 360, 407, 455, 498]\n",
      "51 [29, 77, 121, 173, 216, 266, 308, 360, 405, 456, 500]\n",
      "52 [28, 76, 120, 174, 216, 267, 309, 360, 406, 455, 499]\n",
      "53 [27, 76, 121, 168, 212, 267, 310, 358, 403, 456, 498]\n",
      "54 [28, 76, 121, 168, 218, 268, 310, 361, 405, 459]\n",
      "55 [34, 75, 121, 168, 217, 263, 308, 361, 403, 453, 497]\n",
      "56 [30, 74, 120, 167, 215, 264, 310, 360, 404, 451, 497]\n",
      "57 [27, 75, 121, 168, 215, 264, 313, 359, 401, 453, 497]\n",
      "58 [26, 75, 120, 169, 215, 271, 313, 358, 399, 452, 493]\n",
      "59 [27, 76, 122, 170, 215, 263, 311, 357, 401, 453, 498]\n",
      "60 [26, 72, 120, 170, 217, 261, 309, 358, 403, 452, 498]\n",
      "61 [26, 74, 121, 171, 219, 262, 307, 358, 403, 451, 498]\n",
      "62 [24, 73, 118, 173, 214, 263, 306, 359, 403, 454, 499]\n"
     ]
    }
   ],
   "source": [
    "print(results['image']['max_peaks_positions'])\n",
    "for i in range(0,len(results['slices'])):\n",
    "    array = results['slices'][i]['max_peaks_positions']\n",
    "    print(i,array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(0,len(results['slices'])):\n",
    "    array = results['slices'][i]['intensities']\n",
    "    plt.plot(array) # plotting by columns\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRUEBA ANALISIS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En esta celda se puede probar la funcion de analisis todas las imagenes .tif en un directorio\n",
    "import os\n",
    "from lib.image import image_data\n",
    "from lib.analysis import dyssynchrony_analysis\n",
    "import cv2\n",
    "\n",
    "# Ancho de la feta a analisar (es un parametro opcional)\n",
    "slice_width = 5\n",
    "\n",
    "# Distancia minima en pixeles entre picos\n",
    "min_dist_between_maxs = 200\n",
    "\n",
    "# Calibracion del tiempo de cada pixel\n",
    "calibration = 3.1\n",
    "\n",
    "# Path donde estan las imagenes\n",
    "path = \"C:/Users/Leand/OneDrive/Documentos/Lean/Analizador_imagenes_calcio/1 Hz/\"\n",
    "\n",
    "images_paths = [ f'{path}/{file}' for file in os.listdir(path) if file.endswith(\".tif\") ]\n",
    "images = [ cv2.imread(image_path) for image_path in images_paths ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fromCenter = False\n",
    "showCrosshair = False\n",
    "\n",
    "#Tomamos la primer imagen para seleccionar el recorte elegido el resto de las imagenes se van a recortar igual\n",
    "seleted_parameters = cv2.selectROI(images[0], fromCenter, showCrosshair)\n",
    "\n",
    "# Crop image\n",
    "x_start = int(seleted_parameters[1])\n",
    "x_end = x_start + int(seleted_parameters[3])\n",
    "y_start = int(seleted_parameters[0])\n",
    "y_end = y_start + int(seleted_parameters[2])\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [ image_data.get_image_data(image) for image in images ]\n",
    "images = [ image_data.crop_vertical(image, x_start, x_end) for image in images ]\n",
    "images = [ image_data.crop_horizontal(image, y_start, y_end) for image in images ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [(dyssynchrony_analysis.analyze_image(image, min_dist_between_maxs, calibration, slice_width=5), print ('Done')) for image in images]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis de datos\n",
    "Notas: \n",
    "- Image corresponde al análisis de toda la célula y slices corresponde a fetas de esa imagen\n",
    "- intensidades es el valor que resulta de comprimir (sumando) la matriz con las coordenadas de x,y,z de los pixeles \n",
    "- max_peaks_pos es la index de en la lista de intensidades \n",
    "- max_peaks_intensities el valor de intensidad que se corresponde con el pico en la posicion analoga de max_peaks_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encodes a dictionary into a jason\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Special json encoder for numpy types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "\n",
    "def jasonGenerator(path,results,photo_name):\n",
    "    \"\"\" Generates a .jason file in the path given folder from the a dictionary \"\"\"\n",
    "    with open(path + '/analysis_result_' + photo_name + '.json',\"w\") as miarch:\n",
    "        miarch.write(json.dumps(results, cls=NumpyEncoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jasonReader(path):\n",
    "    \"\"\" Reads the .jason generated with jasonGenartor the given path\"\"\"\n",
    "    with open(path,'r') as miarch:\n",
    "        loaded_dict = json.loads(miarch.read())\n",
    "    return loaded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicesParser_max_peaksI(dictres):\n",
    "#extract all the slices data\n",
    "    allSlices = dictres['slices']\n",
    "    #seting columns names\n",
    "    column_names = ['transient' + str(x) for x in range(0,len(allSlices[0]['max_peaks_intensities']))]    \n",
    "    #define a dataframe\n",
    "    df_sum = pd.DataFrame(columns=column_names)\n",
    "    for i in range(0,len(allSlices)):\n",
    "        df_sum.loc[i] = allSlices[i]['max_peaks_intensities'][0:] \n",
    "    return df_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicesParser_halfpeakstime(dictres):\n",
    "    #extract all the slices data\n",
    "    allSlices = dictres['slices']\n",
    "    \n",
    "    #seting columns names\n",
    "    column_names = ['transient' + str(x+1) for x in range(0,len(allSlices[0]['times_to_half_peaks']))]\n",
    "    #define a dataframe\n",
    "    df_sum = pd.DataFrame(columns=column_names)\n",
    "    #populating the dataframe\n",
    "    for i in range(0,len(allSlices)):\n",
    "        df_sum.loc[i+1] = allSlices[i]['times_to_half_peaks']\n",
    "    return df_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicesParser_amplitudes(dictres):\n",
    "    #extract all the slices data\n",
    "    allSlices = dictres['slices']\n",
    "    #seting columns names\n",
    "    column_names = ['transient' + str(x+1) for x in range(0,len(allSlices[0]['amplitudes']))]\n",
    "    #define a dataframe\n",
    "    df_sum = pd.DataFrame(columns=column_names)\n",
    "    #populating the dataframe\n",
    "    for i in range(0,len(allSlices)):\n",
    "        df_sum.loc[i+1] = allSlices[i]['amplitudes']\n",
    "    return df_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicesParser_amplitudes_col(dictres):\n",
    "    #extract all the slices data\n",
    "    allSlices = dictres['slices']\n",
    "    #seting columns names\n",
    "    column_names = ['transient' + str(x+1) for x in range(0,len(allSlices[0]['amplitudes']))]\n",
    "    #define a dataframe\n",
    "    df_sum = pd.DataFrame(columns=column_names)\n",
    "    #populating the dataframe\n",
    "    for i in range(0,len(allSlices)):\n",
    "        df_sum = pd.concat([df_sum,pd.DataFrame([jasonReader(path_file)['slices'][i]['amplitudes']])], ignore_index=True, axis=0)\n",
    "    print(df_sum)\n",
    "    return df_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicesParser_min_peaksI(dictres):\n",
    "    #extract all the slices data\n",
    "    allSlices = dictres['slices']\n",
    "    #seting columns names\n",
    "    column_names = ['transient' + str(x+1) for x in range(0,len(allSlices[0]['min_peaks_intensities']))]\n",
    "    #define a dataframe\n",
    "    df_sum = pd.DataFrame(columns=column_names)\n",
    "    #populating the dataframe\n",
    "    for i in range(0,len(allSlices)-1):\n",
    "        df_sum.loc[i+1] = allSlices[i]['min_peaks_intensities']\n",
    "\n",
    "    return df_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicesParser_peaktime(dictres):\n",
    "    #extract all the slices data\n",
    "    allSlices = dictres['slices']\n",
    "\n",
    "    #seting columns names\n",
    "    column_names = ['transient' + str(x+1) for x in range(0,len(allSlices[0]['times_to_peaks']))]\n",
    "    #define a dataframe\n",
    "    df_sum = pd.DataFrame(columns=column_names)\n",
    "    #populating the dataframe\n",
    "    for i in range(0,len(allSlices)):\n",
    "        df_sum.loc[i+1] = allSlices[i]['times_to_peaks']\n",
    "\n",
    "    return df_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicesParser_tau(dictres):\n",
    "    #extract all the slices data\n",
    "    allSlices = dictres['slices']\n",
    "    #seting columns names\n",
    "    column_names = ['transient' + str(x+1) for x in range(0,len(allSlices[0]['tau_s']))]\n",
    "    #define a dataframe\n",
    "    df_sum = pd.DataFrame(columns=column_names)\n",
    "    #populating the dataframe\n",
    "    for i in range(0,len(allSlices)):\n",
    "        df_sum.loc[i+1] = allSlices[i]['tau_s']\n",
    "\n",
    "    return df_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BasicStasts(df):\n",
    "    if df.empty == False:\n",
    "        return df.astype('int').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amplitudes_ratio(df):\n",
    "    df_alt = pd.DataFrame()\n",
    "    for i in range(0,len(list(df))-1):\n",
    "        name = 'ratio' + str(i+1) +'-' + str(i)\n",
    "        df_alt[name] = df[list(df)[i+1]]/df[list(df)[i]]\n",
    "    return df_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/leandro/Documentos/Analisis_de_imagenes/CardiAP/DesktopApp/photos_examples/'\n",
    "photo_name = 'c1d000'\n",
    "photo = photo_name + '.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dict = jasonReader(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BasicStasts(slicesParser_max_peaksI(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitudes_ratio(slicesParser_amplitudes(path)).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_mean = slicesParser_amplitudes(path).mean()\n",
    "for i in range(0,len(amp_mean)-1):\n",
    "    AR_ind = (1- (amp_mean[i+1])/amp_mean[i])\n",
    "    print (AR_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slicesParser_min_peaksI(path).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slicesParser_tau(path).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discordance_index(path):\n",
    "    DIs = pd.DataFrame()\n",
    "    for j in range (1, len(slicesParser_amplitudes(path).columns)):\n",
    "        rel_diff = []\n",
    "        for i in range (0, len (slicesParser_amplitudes(path))):\n",
    "            T1 = list(slicesParser_amplitudes(path)['transient'+str(j)])[i]\n",
    "            T2 = list(slicesParser_amplitudes(path)['transient'+str(j+1)])[i]\n",
    "            rel_diff.append((T1-T2)/max(T1,T2))\n",
    "        DIs[j] = rel_diff\n",
    "    return DIs\n",
    "def alternance_ratio(path):\n",
    "    T1_mean = float(slicesParser_amplitudes(path)['transient1'].mean())\n",
    "    T2_mean = float(slicesParser_amplitudes(path)['transient2'].mean())\n",
    "    AR = (abs(T1_mean-T2_mean))/max(T1_mean, T2_mean)\n",
    "    return AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DI_list = []\n",
    "path = '/media/leandro/Volumen1TB/Lean/Analizador_imagenes_calcio/Luis/analisis/json_nuevos/'\n",
    "jsons_list = os.listdir(path)\n",
    "file_error = []\n",
    "for file in jsons_list:\n",
    "    try:\n",
    "        path_file = path + file\n",
    "        dic = jasonReader(path_file)\n",
    "        print (file)\n",
    "        DI_list.append([file, discordance_index(dic).std().mean()])\n",
    "    except ValueError:\n",
    "        file_error.append(file)\n",
    "    except IsADirectoryError:\n",
    "        pass\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = []\n",
    "for index in range(0,len(DI_list)):\n",
    "    file.append([jsons_list[index], (DI_list[index]).std().mean()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd    \n",
    "\n",
    "df = pd.DataFrame(DI_list, columns= ['photo_name','DI'])\n",
    "df.to_csv(path + 'discordances_indexes_nuevos.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'Failed.txt', 'w') as file:\n",
    "    for item in file_error:\n",
    "        file.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_defectuosos = []\n",
    "for i in list_archs:\n",
    "    dic = jasonReader(i)\n",
    "    try:\n",
    "        slicesParser_max_peaksI(dic)\n",
    "    except:\n",
    "        arch_defectuosos.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_defectuosos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "AR_list = []\n",
    "path = '/media/leandro/Volumen1TB/Lean/Analizador_imagenes_calcio/Luis/analisis/json_nuevos/'\n",
    "jsons_list = os.listdir(path)\n",
    "for file in jsons_list:\n",
    "    try:\n",
    "        path_file = path + file\n",
    "        dic = jasonReader(path_file)\n",
    "        T1_mean = dic['image']['amplitudes'][0]\n",
    "        T2_mean = dic['image']['amplitudes'][1]\n",
    "        AR_list.append([file, (abs(T1_mean-T2_mean))/max(T1_mean, T2_mean)])\n",
    "    except IsADirectoryError:\n",
    "        pass\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd    \n",
    "\n",
    "df = pd.DataFrame(AR_list, columns= ['photo_name','AR'])\n",
    "df.to_csv(path + 'alternance_ratio_nuevos.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_index = pd.read_csv('/media/leandro/Volumen1TB/Lean/Analizador_imagenes_calcio/Luis/analisis/json_nuevos/discordances_indexes_nuevos.csv', encoding='utf-8', sep='\\t')\n",
    "del disc_index['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, ttest_ind_from_stats\n",
    "c_1 = disc_index[(disc_index['tratamiento'] == 'c') & (disc_index['frecuencia'] == 1)]\n",
    "del c_1['tratamiento']\n",
    "del c_1['frecuencia']\n",
    "c_3 = disc_index[(disc_index['tratamiento'] == 'c') & (disc_index['frecuencia'] == 3)]\n",
    "del c_3['tratamiento']\n",
    "del c_3['frecuencia']\n",
    "c_4 = disc_index[(disc_index['tratamiento'] == 'c') & (disc_index['frecuencia'] == 4)]\n",
    "del c_4['tratamiento']\n",
    "del c_4['frecuencia']\n",
    "c_5 = disc_index[(disc_index['tratamiento'] == 'c') & (disc_index['frecuencia'] == 5)]\n",
    "del c_5['tratamiento']\n",
    "del c_5['frecuencia']\n",
    "\n",
    "\n",
    "df1 = pd.concat([c_1.reset_index(),c_3.reset_index(),c_4.reset_index(),c_5.reset_index()],axis =1, ignore_index=True, sort=False)\n",
    "df1.to_csv('/media/leandro/Volumen1TB/Lean/Analizador_imagenes_calcio/Luis/analisis/json_nuevos/DI_nuevos.csv', encoding='utf-8', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DI = pd.read_csv('/media/leandro/Volumen1TB/Lean/Analizador_imagenes_calcio/Luis/analisis/json_nuevos/DI_nuevos.csv', encoding='utf-8', sep='\\t')\n",
    "DI.head()\n",
    "del DI['photo_name']\n",
    "\n",
    "from mpl_toolkits.axisartist.axislines import SubplotZero\n",
    "fig = plt.figure()\n",
    "ax = SubplotZero(fig, 111)\n",
    "fig.add_subplot(ax)\n",
    "\n",
    "# for direction in [\"xzero\", \"yzero\"]:\n",
    "#     # adds arrows at the ends of each axis\n",
    "#     ax.axis[direction].set_axisline_style(\"-|>\")\n",
    "\n",
    "#     # adds X and Y-axis from the origin\n",
    "#     ax.axis[direction].set_visible(False)\n",
    "\n",
    "for direction in [\"right\", \"top\"]:\n",
    "    # hides borders\n",
    "    ax.axis[direction].set_visible(False)\n",
    "    \n",
    "for row in range(0,len(DI)):\n",
    "    plt.plot(DI.loc[row],'o''-')\n",
    "    plt.box(on=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, ttest_ind_from_stats\n",
    "c_1 = disc_index[(disc_index['tratamiento'] == 'c') & (disc_index['frecuencia'] == 1)]['DI']\n",
    "c_3 = disc_index[(disc_index['tratamiento'] == 'c') & (disc_index['frecuencia'] == 3)]['DI']\n",
    "c_4 = disc_index[(disc_index['tratamiento'] == 'c') & (disc_index['frecuencia'] == 4)]['DI']\n",
    "c_5 = disc_index[(disc_index['tratamiento'] == 'c') & (disc_index['frecuencia'] == 5)]['DI']\n",
    "\n",
    "\n",
    "df = pd.concat([c_1.reset_index()['DI'],c_3.reset_index()['DI'],c_4.reset_index()['DI'],c_5.reset_index()['DI']],axis =1, ignore_index=True, sort=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DI = DI.interpolate(method ='linear', limit_direction ='forward')\n",
    "t, p = ttest_ind(DI['4'], DI['5'], equal_var=False)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "# Initialize the figure with a logarithmic x axis\n",
    "f, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "\n",
    "sns.boxplot(data=DI)\n",
    "\n",
    "sns.swarmplot(data=DI, linewidth=2)\n",
    "# plt.savefig('AR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "tau_list = []\n",
    "path = 'C:/Users/Leand/OneDrive/Documentos/Lean/Analizador_imagenes_calcio/Luis/jsons/'\n",
    "jsons_list = os.listdir(path)\n",
    "for file in jsons_list:\n",
    "    path_file = path + file\n",
    "    dic = jasonReader(path_file)\n",
    "    taus = slicesParser_tau(dic)\n",
    "    tau_list.append(taus)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_list[-1].mean().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_list = []\n",
    "path = 'C:/Users/Leand/OneDrive/Documentos/Lean/Analizador_imagenes_calcio/Luis/jsons/'\n",
    "jsons_list = os.listdir(path)\n",
    "for file in jsons_list:\n",
    "    path_file = path + file\n",
    "    dic = jasonReader(path_file)\n",
    "    taus = dic['image']['tau_s']\n",
    "    tau_list.append([file,sum(taus)/len(taus),slicesParser_tau(dic).mean().mean()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd    \n",
    "\n",
    "df = pd.DataFrame(tau_list, columns= ['photo_name','tau_wc', 'tau_slices'])\n",
    "df.to_csv(path + 'tau.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pandas import Series\n",
    "\n",
    "from lmfit import Model, Parameter, report_fit\n",
    "\n",
    "def decay(t, N, tau):\n",
    "    return N*np.exp(-t/tau)\n",
    "\n",
    "t = np.linspace(218, 244, num=26)\n",
    "data = [50, 50, 50, 50, 50, 50, 50, 49, 49, 49, 49, 49, 48, 48, 48, 48, 47, 47, 47, 46, 46, 46, 45, 45, 45, 45]\n",
    "\n",
    "model = Model(decay, independent_vars=['t'])\n",
    "result = model.fit(data, t=t, N=10, tau=150)\n",
    "result.plot()\n",
    "print(result.values)\n",
    "\n",
    "result = model.fit(data, t=t,\n",
    "                   N=Parameter('N', value=10),\n",
    "                   tau=Parameter('tau', value=1))\n",
    "report_fit(result.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
