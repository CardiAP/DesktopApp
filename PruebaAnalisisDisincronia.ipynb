{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En esta celda se puede probar la funcion de analisis de una imagen\n",
    "\n",
    "from lib.image import image_data\n",
    "from lib.analysis import dyssynchrony_analysis\n",
    "import cv2\n",
    "\n",
    "# Path completo donde esta la imagen (incluyendo nombre y extencion)\n",
    "path = '/media/leandro/20d94040-1ffc-4389-948c-746fd67f15fe/Lean/Analizador_imagenes_calcio/230819sel/230819Dan2/'\n",
    "photo_name = 'D2d000'\n",
    "photo = photo_name + '.tif'\n",
    "# Ancho de la feta a analisar (es un parametro opcional)\n",
    "slice_width = 5\n",
    "\n",
    "# Distancia minima en pixeles entre picos\n",
    "min_dist_between_maxs = 30\n",
    "# Calibracion del tiempo de cada pixel\n",
    "calibration = 4.5\n",
    "\n",
    "image = cv2.imread(path + photo_name + '.tif')\n",
    "# Select ROI\n",
    "fromCenter = False\n",
    "showCrosshair = False\n",
    "seleted_parameters = cv2.selectROI(image, fromCenter, showCrosshair)\n",
    "\n",
    "# Crop image\n",
    "x_start = int(seleted_parameters[1])\n",
    "x_end = x_start + int(seleted_parameters[3])\n",
    "y_start = int(seleted_parameters[0])\n",
    "y_end = y_start + int(seleted_parameters[2])\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "image = image_data.get_image_data(image)\n",
    "image = image_data.crop_vertical(image, x_start, x_end)\n",
    "image = image_data.crop_horizontal(image, y_start, y_end)\n",
    "\n",
    "results = dyssynchrony_analysis.analyze_image(image, min_dist_between_maxs, calibration, slice_width=slice_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jasonGenerator(path,results,photo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slicesParser_amplitudes(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results['image']['max_peaks_positions'])\n",
    "for i in range(0,len(results['slices'])):\n",
    "    array = results['slices'][i]['max_peaks_positions']\n",
    "    print(i,array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(0,len(results['slices'])):\n",
    "    array = results['slices'][i]['intensities']\n",
    "    plt.plot(array) # plotting by columns\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRUEBA ANALISIS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En esta celda se puede probar la funcion de analisis todas las imagenes .tif en un directorio\n",
    "import os\n",
    "from lib.image import image_data\n",
    "from lib.analysis import dyssynchrony_analysis\n",
    "import cv2\n",
    "\n",
    "# Ancho de la feta a analisar (es un parametro opcional)\n",
    "slice_width = 5\n",
    "\n",
    "# Distancia minima en pixeles entre picos\n",
    "min_dist_between_maxs = 200\n",
    "\n",
    "# Calibracion del tiempo de cada pixel\n",
    "calibration = 3.1\n",
    "\n",
    "# Path donde estan las imagenes\n",
    "path = \"C:/Users/Leand/OneDrive/Documentos/Lean/Analizador_imagenes_calcio/1 Hz/\"\n",
    "\n",
    "images_paths = [ f'{path}/{file}' for file in os.listdir(path) if file.endswith(\".tif\") ]\n",
    "images = [ cv2.imread(image_path) for image_path in images_paths ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fromCenter = False\n",
    "showCrosshair = False\n",
    "\n",
    "#Tomamos la primer imagen para seleccionar el recorte elegido el resto de las imagenes se van a recortar igual\n",
    "seleted_parameters = cv2.selectROI(images[0], fromCenter, showCrosshair)\n",
    "\n",
    "# Crop image\n",
    "x_start = int(seleted_parameters[1])\n",
    "x_end = x_start + int(seleted_parameters[3])\n",
    "y_start = int(seleted_parameters[0])\n",
    "y_end = y_start + int(seleted_parameters[2])\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [ image_data.get_image_data(image) for image in images ]\n",
    "images = [ image_data.crop_vertical(image, x_start, x_end) for image in images ]\n",
    "images = [ image_data.crop_horizontal(image, y_start, y_end) for image in images ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [(dyssynchrony_analysis.analyze_image(image, min_dist_between_maxs, calibration, slice_width=5), print ('Done')) for image in images]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis de datos\n",
    "Notas: \n",
    "- Image corresponde al análisis de toda la célula y slices corresponde a fetas de esa imagen\n",
    "- intensidades es el valor que resulta de comprimir (sumando) la matriz con las coordenadas de x,y,z de los pixeles \n",
    "- max_peaks_pos es la index de en la lista de intensidades \n",
    "- max_peaks_intensities el valor de intensidad que se corresponde con el pico en la posicion analoga de max_peaks_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encodes a dictionary into a jason\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" Special json encoder for numpy types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "\n",
    "def jasonGenerator(path,results,photo_name):\n",
    "    \"\"\" Generates a .jason file in the path given folder from the a dictionary \"\"\"\n",
    "    with open(path + '/analysis_result_' + photo_name + '.json',\"w\") as miarch:\n",
    "        miarch.write(json.dumps(results, cls=NumpyEncoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jasonReader(path):\n",
    "    \"\"\" Reads the .jason generated with jasonGenartor the given path\"\"\"\n",
    "    with open( path,'r') as miarch:\n",
    "        loaded_dict = json.loads(miarch.read())\n",
    "    return loaded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicesParser_max_peaksI(path):\n",
    "#     read the analysis result \n",
    "    dictres = jasonReader(path)\n",
    "\n",
    "#extract all the slices data\n",
    "    allSlices = dictres['slices']\n",
    "    try:\n",
    "        #seting columns names\n",
    "        column_names = ['transient' + str(x) for x in range(0,len(allSlices[0]['max_peaks_intensities']))]    \n",
    "        #define a dataframe\n",
    "        df_sum = pd.DataFrame(columns=column_names)\n",
    "        for i in range(0,len(allSlices)):\n",
    "            df_sum.loc[i] = allSlices[i]['max_peaks_intensities'][0:] \n",
    "        return df_sum\n",
    "\n",
    "    except:\n",
    "        print('no possible callculation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicesParser_halfpeakstime(path):\n",
    "    #read the analysis result \n",
    "    dictres = jasonReader(path)\n",
    "    #extract all the slices data\n",
    "    allSlices = dictres['slices']\n",
    "#     try:\n",
    "    #seting columns names\n",
    "    column_names = ['transient' + str(x+1) for x in range(0,len(allSlices[0]['times_to_half_peaks']))]\n",
    "    #define a dataframe\n",
    "    df_sum = pd.DataFrame(columns=column_names)\n",
    "    #populating the dataframe\n",
    "    for i in range(0,len(allSlices)):\n",
    "        df_sum.loc[i+1] = allSlices[i]['times_to_half_peaks']\n",
    "    return df_sum\n",
    "\n",
    "#     except:\n",
    "#         print('no possible callculation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicesParser_amplitudes(path):\n",
    "    #read the analysis result \n",
    "    dictres = jasonReader(path)\n",
    "    #extract all the slices data\n",
    "    allSlices = dictres['slices']\n",
    "    try:\n",
    "        #seting columns names\n",
    "        column_names = ['transient' + str(x+1) for x in range(0,len(allSlices[0]['amplitudes']))]\n",
    "        #define a dataframe\n",
    "        df_sum = pd.DataFrame(columns=column_names)\n",
    "        #populating the dataframe\n",
    "        for i in range(0,len(allSlices)):\n",
    "            df_sum.loc[i+1] = allSlices[i]['amplitudes']\n",
    "\n",
    "        return df_sum\n",
    "    \n",
    "    except:\n",
    "        print('no possible callculation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicesParser_min_peaksI(path):\n",
    "    #read the analysis result \n",
    "    dictres = jasonReader(path)\n",
    "    #extract all the slices data\n",
    "    allSlices = dictres['slices']\n",
    "    try:\n",
    "        #seting columns names\n",
    "        column_names = ['transient' + str(x+1) for x in range(0,len(allSlices[0]['min_peaks_intensities']))]\n",
    "        #define a dataframe\n",
    "        df_sum = pd.DataFrame(columns=column_names)\n",
    "        #populating the dataframe\n",
    "        for i in range(0,len(allSlices)):\n",
    "            df_sum.loc[i+1] = allSlices[i]['min_peaks_intensities']\n",
    "\n",
    "        return df_sum\n",
    "    \n",
    "    except:\n",
    "        print('no possible callculation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicesParser_peaktime(path):\n",
    "    #read the analysis result \n",
    "    dictres = jasonReader(path)\n",
    "    #extract all the slices data\n",
    "    allSlices = dictres['slices']\n",
    "\n",
    "    try:\n",
    "        #seting columns names\n",
    "        column_names = ['transient' + str(x+1) for x in range(0,len(allSlices[0]['times_to_peaks']))]\n",
    "        #define a dataframe\n",
    "        df_sum = pd.DataFrame(columns=column_names)\n",
    "        #populating the dataframe\n",
    "        for i in range(0,len(allSlices)):\n",
    "            df_sum.loc[i+1] = allSlices[i]['times_to_peaks']\n",
    "\n",
    "        return df_sum\n",
    "    \n",
    "    except:\n",
    "        print('no possible callculation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicesParser_tau(path):\n",
    "    #read the analysis result \n",
    "    dictres = jasonReader(path)\n",
    "    #extract all the slices data\n",
    "    allSlices = dictres['slices']\n",
    "    #seting columns names\n",
    "    try:\n",
    "        column_names = ['transient' + str(x+1) for x in range(0,len(allSlices[0]['tau_s']))]\n",
    "        #define a dataframe\n",
    "        df_sum = pd.DataFrame(columns=column_names)\n",
    "        #populating the dataframe\n",
    "        for i in range(0,len(allSlices)):\n",
    "            df_sum.loc[i+1] = allSlices[i]['tau_s']\n",
    "        \n",
    "        return df_sum\n",
    "    \n",
    "    except:\n",
    "        print('no possible callculation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BasicStasts(df):\n",
    "    if df.empty == False:\n",
    "        return df.astype('int').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def amplitudes_ratio(df):\n",
    "    df_alt = pd.DataFrame()\n",
    "    for i in range(0,len(list(df))-1):\n",
    "        name = 'ratio' + str(i+1) +'-' + str(i)\n",
    "        df_alt[name] = df[list(df)[i+1]]/df[list(df)[i]]\n",
    "    return df_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/leandro/Documentos/Analisis_de_imagenes/CardiAP/DesktopApp/photos_examples/'\n",
    "photo_name = 'c1d000'\n",
    "photo = photo_name + '.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_dict = jasonReader(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BasicStasts(slicesParser_max_peaksI(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitudes_ratio(slicesParser_amplitudes(path)).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_mean = slicesParser_amplitudes(path).mean()\n",
    "for i in range(0,len(amp_mean)-1):\n",
    "    AR_ind = (1- (amp_mean[i+1])/amp_mean[i])\n",
    "    print (AR_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no possible callculation\n"
     ]
    }
   ],
   "source": [
    "slicesParser_min_peaksI(path).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slicesParser_tau(path).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discordance_index(path):\n",
    "    DIs = pd.DataFrame()\n",
    "    for j in range (1, len(slicesParser_amplitudes(path).columns)):\n",
    "        rel_diff = []\n",
    "        for i in range (0, len (slicesParser_amplitudes(path))):\n",
    "            T1 = list(slicesParser_amplitudes(path)['transient'+str(j)])[i]\n",
    "            T2 = list(slicesParser_amplitudes(path)['transient'+str(j+1)])[i]\n",
    "            rel_diff.append((T1-T2)/max(T1,T2))\n",
    "        DIs[j] = rel_diff\n",
    "    return DIs\n",
    "def alternance_ratio(path):\n",
    "    ARs = []\n",
    "    for j in range (1, len(slicesParser_amplitudes(path).columns)):\n",
    "        T1_mean = float(slicesParser_amplitudes(path)['transient'+str(j)].mean())\n",
    "        T2_mean = float(slicesParser_amplitudes(path)['transient'+str(j+1)].mean())\n",
    "        ARs.append((T1_mean-T2_mean)/T1_mean)\n",
    "    return ARs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "AR_mean = sum(AR)/len(AR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no possible callculation\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-7cd9ec1fdfd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mjsons_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mpath_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mslicesParser_max_peaksI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m#     discordance_index(path_file)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-fa2e9ea335ea>\u001b[0m in \u001b[0;36mslicesParser_max_peaksI\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#extract all the slices data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mallSlices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdictres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'slices'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m#seting columns names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "path = 'C:/Users/Leand/OneDrive/Documentos/Lean/Analizador_imagenes_calcio/Luis/JSONs/'\n",
    "jsons_list = os.listdir(path)\n",
    "for file in jsons_list:\n",
    "    path_file = path + file\n",
    "    slicesParser_max_peaksI(path_file)\n",
    "#     discordance_index(path_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def slicesParser_max_peaksI(path):\n",
    "#     read the analysis result \n",
    "dictres = jasonReader(path_file)\n",
    "\n",
    "#extract all the slices data\n",
    "    allSlices = dictres[i]['slices']\n",
    "    #     try:\n",
    "    #seting columns names\n",
    "    column_names = ['transient' + str(x) for x in range(0,len(allSlices[0]['max_peaks_intensities']))]    \n",
    "    #define a dataframe\n",
    "    df_sum = pd.DataFrame(columns=column_names)\n",
    "    for i in range(0,len(allSlices)):\n",
    "        df_sum.loc[i] = allSlices[i]['max_peaks_intensities'][0:] \n",
    "#         return df_sum\n",
    "\n",
    "#     except:\n",
    "#         print('no possible callculation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transient0</th>\n",
       "      <th>transient1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   transient0 transient1\n",
       "0          18         17\n",
       "1          19         18\n",
       "2          19         18\n",
       "3          19         18\n",
       "4          19         18\n",
       "5          19         18\n",
       "6          19         18\n",
       "7          20         18\n",
       "8          20         19\n",
       "9          21         19\n",
       "10         21         20\n",
       "11         21         20\n",
       "12         21         21\n",
       "13         20         21\n",
       "14         20         21\n",
       "15         21         21\n",
       "16         21         22\n",
       "17         22         22\n",
       "18         22         22\n",
       "19         21         22\n",
       "20         21         21\n",
       "21         21         21\n",
       "22         20         21\n",
       "23         20         21\n",
       "24         20         20\n",
       "25         19         20\n",
       "26         19         19\n",
       "27         18         18\n",
       "28         18         18\n",
       "29         17         17\n",
       "30         18         17\n",
       "31         18         18"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
